import numpy as np

import pyemma.util.linalg as pla

class LowRank:
    ''' This structure represents a low-rank representation for the component
    tensor in one step of ALS. It comprises all methods to set and access the
    low-rank components and to optimize them.
            
    Parameters:
    ------------
    U0,U1: ndarray, shape(n0,R) and (n1,R), where n0 = r_p-1*n, n1= rp*M, and 
        R is the rank of the representation.
    dims: triple of the dimensions (r_p-1,n,r_p).
    Ctau, C0: ndarrays, shape(r_p-1*n*r_p,r_p-1*n*r_p), correlation matrices.
    '''
    def __init__(self,U0,U1,dims,Ctau,C0):
        # Define attributes:
        self.R = U0.shape[1]
        self.n0 = U0.shape[0]
        self.n1 = U1.shape[0]
        self.U0 = U0
        self.U1 = U1
        self.Ctau = Ctau
        self.C0 = C0
        # Also extract the dimensions of the high-dimensional representation:
        self.Np = Ctau.shape[0]
        self.M = U1.shape[0]/dims[2]
        # Generate the accordingly shaped  high-dimensional repr. U:
        U = np.dot(self.U0,self.U1.transpose())
        self.U = np.reshape(U,(self.Np,self.M))
    def GetComponents(self):
        ''' Return the low-rank components U0,U1:
                
        Returns:
        ----------
        list, containing the arrays [U0,U1].
        '''
        complist = [self.U0,self.U1]
        return complist
    def SetComponents(self,U0,U1):
        ''' Set both low-rank components to a new value:
        
        Parameters:
        -------------
        U0,U1: ndarrays, shape(n0,R) and (n1,R), the new low-rank components.
        '''
        self.U0 = U0
        self.U1 = U1
        # Update the high-dimensional repr. U:
        U = np.dot(self.U0,self.U1.transpose())
        self.U = np.reshape(U,(self.Np,self.M))
    def GetVector(self):
        ''' Return the low-rank variables as a vector.
        
        Returns:
        ------------
        ndarray, shape(R*(n0+n1),).
        '''
        # Reshape the components and insert them into the vector. By the order
        # used in the optimization algorithm, we have to transpose U0 and U1 in
        # order to have the ranks varying most slowly. 
        u = np.zeros(self.R*(self.n0+self.n1))
        u[:self.R*self.n0] = np.reshape(self.U0.transpose(),(self.R*self.n0,))
        u[self.R*self.n0:] = np.reshape(self.U1.transpose(),(self.R*self.n1,))
        return u
    def SetVector(self,u):
        ''' Update the components from a vector:
        
        Parameters:
        ------------
        u: ndarray, shape(R*(n0+n1),), the new low-rank components in a vector.
        '''
        # Extract the components:
        U0 = np.reshape(u[:self.R*self.n0],(self.R,self.n0))
        U1 = np.reshape(u[self.R*self.n0:],(self.R,self.n1))
        # Insert them
        self.SetComponents(U0.transpose(),U1.transpose())
    def Timescales(self,tau):
        ''' Compute the implied timescales generated by the current representa-
        tion of U.
        
        Parameters:
        ------------
        tau: int. the lagtime for implied timescales.
        
        Returns:
        ------------
        ndarray, shape(self.M-1,), array of the first M-1 implied timescales,
        in descending order.
        '''
        # Compute the correlation-matrices of the current estimates:
        Ct = np.dot(self.U.transpose(),np.dot(self.Ctau,self.U))
        C0 = np.dot(self.U.transpose(),np.dot(self.C0,self.U))
        # Solve generalized eigenvalue problem:
        D,_ = pla.eig_corr(C0,Ct)
        # Compute timescales:
        return -self.tau/np.log(D[1:])
    def Objective(self):
        ''' Compute the (unconstrained) objective function (eigenvalue trace)
        for the current components.

        Returns:
        -----------
        float.
        '''
        L = -0.5*np.trace(np.dot(self.U.transpose(),np.dot(self.Ctau,self.U)))
        return L
    def ContrainedObjective(self,mu):
        ''' Compute the constrained objective function (eigenvalue trace + qua-
        dratic penalty terms) for the current components.
        
        Parameters:
        -----------
        mu:  float, the penalty parameter for the constraints. 
        
        Returns:
        -----------
        float.
        '''
        # Compute eigenvalue trace:
        L = -0.5*np.trace(np.dot(self.U.transpose(),np.dot(self.Ctau,self.U)))
        # Add the constraints:
        q = self.Penalty(mu)
        return (L + q)
    def Penalty(self,mu):
        ''' Evaluate penalty function for current components.
        
        Parameters:
        -----------
        mu:  float, the penalty parameter for the constraints. 
        
        Returns:
        -----------
        float.
        '''
        # Compute constraints:
        Q = self.Orthogonality()
        # Sum them up:
        q = 0
        for s in range(self.M):
            for t in range(s,self.M):
                q += 0.5*mu*Q[s,t]**2
        return q
    def Orthogonality(self):
        ''' Evaluate the orthogonality of the current solution.
        
        Returns:
        ------------
        ndarray, shape(self.M,self.M), the overlaps between the functions U(:,n)
            and U(:,n').
        '''
        return np.dot(self.U.transpose(),np.dot(self.C0,self.U)) - np.eye(self.M)
    def Gradient(self,mu):
        ''' Compute the gradient of the constrained functional w.r.t. the low-
        rank variables.
        
        Parameters:
        -------------
        mu:  float, the penalty parameter for the constraints.
        
        Returns:
        -------------
        ndarray, shape(R*(n0+n1),), the gradient vector.
        '''
        # Get the high-dimensional derivatives:
        F = self.FDerivative(mu)
        # Get the low-rank Jacobian:
        J = self.Jacobian()
        # Their product is the result:
        return np.dot(F,J)
    def FDerivative(self,mu):
        ''' Compute the derivatives of the constrained objective function w.r.t.
        the high-dimensional variables.
        
        Parameters:
        -------------
        mu:  float, the penalty parameter for the constraints.
        
        Returns:
        -------------
        ndarray, shape(n0*n1,), the gradient vector.
        '''
        # It is easiest to start in Np-by-M matrix form and reshape at the end:
        DL = np.zeros((self.Np,self.M))
        # Compute derivatives of unconstrained objective first:
        DL += np.dot(self.Ctau,self.U)
        # Now add the constraints one by one:
        Q = self.Orthogonality()
        for s in range(self.M):
            for t in range(s,self.M):
                DL[:,s] += mu*Q[s,t]*np.dot(self.C0,self.U[:,s])
                DL[:,t] += mu*Q[s,t]*np.dot(self.C0,self.U[:,t])
        # Reshape DL to vector form:
        DL = np.reshape(DL,(self.Np*self.M))
        return DL
    def Jacobian(self):
        ''' Return the Jacobian matrix of the map that takes the low-dimen-
        sional components to the high-dimensional full tensor.
        
        Returns:
        DU: nd-array, shape(n0*n1,R*(n0+n1)), each row contains the derivatives
        of the representation w.r.t. one of the low-rank entries.
        '''
        # Prepare output:
        DU = np.zeros((self.n0*self.n1,self.R*(self.n0+self.n1)))
        # First dimension:
        E = np.eye(self.n0,self.n0)
        for r in range(self.R):
            vr = self.U1[r,:]
            for k in range(self.n0):
                ekr = np.outer(E[k,:],vr)
                DU[:,r*self.n0+k] = np.reshape(ekr,(self.n0*self.n1,))
        # Second dimension:
        offset = self.R*self.n0
        E = np.eye(self.n1,self.n1)
        for r in range(self.R):
            vr = self.U0[r,:]
            for k in range(self.n1):
                ekr = np.outer(vr,E[k,:])
                DU[:,offset+r*self.n1+k] = np.reshape(ekr,(self.n0*self.n1,))
        return DU